{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "5a321cc9",
   "metadata": {},
   "source": [
    "# Human or Neural Translation\n",
    "\n",
    "- ## Feature-based\n",
    "\n",
    "Monolingual features:\n",
    "  - n-gram\n",
    "      - 2-7 range, top 30k\n",
    "  - KenLM features \n",
    "      - ratios of min and max logprob over the (target) sentence per model\n",
    "      - the number of tokens with a logprob less than {mean, max, −6} (three features per\n",
    "      - the logprob of the full sentence given by the left-to-right model\n",
    "\n",
    "Bilingual features:\n",
    "  - \"Unsupervised feature\" aggregation for detecting spurious alignment\n",
    "\n",
    "- ## Neural\n",
    "\n",
    "Monolingual features:\n",
    "  - BiLSTM from scratch\n",
    "  - LASER representations\n",
    "  - Pretrained transformers\n",
    "  \n",
    "Bilingual features:\n",
    "  - BiLSTM\n",
    "  - LASER representations (diff concat with dot)\n",
    "  - Pretrained transformers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "7a9c5c65",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "\n",
    "from sacremoses import MosesTokenizer\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "\n",
    "from functools import partial\n",
    "\n",
    "train_df = pd.read_csv('train.tsv', sep='\\t', encoding='utf-8')\n",
    "valid_df = pd.read_csv('valid.tsv', sep='\\t', encoding='utf-8')\n",
    "\n",
    "mt = MosesTokenizer(lang='en')\n",
    "\n",
    "# Join tokens on whitespace so CountVectorizer is happy\n",
    "tokenizer = partial(mt.tokenize, return_str=True)\n",
    "\n",
    "vectorizer = CountVectorizer(tokenizer=tokenizer, max_features=30_000)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "62b970d4",
   "metadata": {},
   "source": [
    "Using Europarl corpus, Danish to English. The `en_mt` column has been populated by filtering non-empty source and target rows and translating Danish using the `Helsinki-NLP/opus-mt-da-en` model via Huggingface."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "209edc2c",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>en</th>\n",
       "      <th>da</th>\n",
       "      <th>en_mt</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>My final point is that animals should not be s...</td>\n",
       "      <td>Afslutningsvis vil jeg sige, at dyr ikke bør u...</td>\n",
       "      <td>In conclusion, animals should not be subjected...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>A clear agreement on this item would have comp...</td>\n",
       "      <td>En klar aftale om dette spørgsmål havde afslut...</td>\n",
       "      <td>A clear agreement on this issue had ended this...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>Thank you very much.</td>\n",
       "      <td>Mange tak.</td>\n",
       "      <td>Thank you very much.</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>As a result of this debate, I would like to co...</td>\n",
       "      <td>Som resultat af denne forhandling vil jeg gern...</td>\n",
       "      <td>As a result of this debate, I would like to co...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>I have little doubt that the report will event...</td>\n",
       "      <td>Jeg tvivler ikke på, at betænkningen med tiden...</td>\n",
       "      <td>I have no doubt that the report will eventuall...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                  en  \\\n",
       "0  My final point is that animals should not be s...   \n",
       "1  A clear agreement on this item would have comp...   \n",
       "2                               Thank you very much.   \n",
       "3  As a result of this debate, I would like to co...   \n",
       "4  I have little doubt that the report will event...   \n",
       "\n",
       "                                                  da  \\\n",
       "0  Afslutningsvis vil jeg sige, at dyr ikke bør u...   \n",
       "1  En klar aftale om dette spørgsmål havde afslut...   \n",
       "2                                         Mange tak.   \n",
       "3  Som resultat af denne forhandling vil jeg gern...   \n",
       "4  Jeg tvivler ikke på, at betænkningen med tiden...   \n",
       "\n",
       "                                               en_mt  \n",
       "0  In conclusion, animals should not be subjected...  \n",
       "1  A clear agreement on this issue had ended this...  \n",
       "2                               Thank you very much.  \n",
       "3  As a result of this debate, I would like to co...  \n",
       "4  I have no doubt that the report will eventuall...  "
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "e168c548",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "def reorganize_data(df, ht_col=\"en\", mt_col=\"en_mt\"):\n",
    "    \"\"\"Combines HT and MT column and assigns 1 to HTs and 0 to MTs.\n",
    "    X and y are then shuffled.\n",
    "    \"\"\"\n",
    "    X_ht = df[ht_col].values\n",
    "    y_ht = np.ones_like(X_ht, dtype=np.int32)\n",
    "    X_mt = df[mt_col].values\n",
    "    y_mt = np.zeros_like(X_mt, dtype=np.int32)\n",
    "    X = np.hstack([X_ht, X_mt])\n",
    "    y = np.hstack([y_ht, y_mt])\n",
    "    assert X.shape == y.shape\n",
    "    # Shuffle the X and y the same way by shuffling indices and indexing\n",
    "    indices = np.arange(X.shape[0])\n",
    "    np.random.shuffle(indices)\n",
    "    return X[indices], y[indices]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "af2dccba",
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train, y_train = reorganize_data(train_df)\n",
    "X_valid, y_valid = reorganize_data(valid_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "4227b3ca",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.pipeline import Pipeline\n",
    "\n",
    "pipe = Pipeline([\n",
    "    (\"feat\", vectorizer),\n",
    "    (\"model\", RandomForestClassifier(n_estimators=1000, max_depth=40))\n",
    "])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "c82e4fa9",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Pipeline(steps=[('feat',\n",
       "                 CountVectorizer(max_features=30000,\n",
       "                                 tokenizer=functools.partial(<bound method MosesTokenizer.tokenize of <sacremoses.tokenize.MosesTokenizer object at 0x000001BC58479430>>, return_str=True))),\n",
       "                ('model',\n",
       "                 RandomForestClassifier(max_depth=40, n_estimators=1000))])"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pipe.fit(X_train, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "c33f2373",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.5509"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pipe.score(X_valid, y_valid)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "1f8af4a6",
   "metadata": {},
   "outputs": [],
   "source": [
    "y_pred = pipe.predict(X_valid)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "d4a2143e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.55      0.59      0.57      5000\n",
      "           1       0.56      0.51      0.53      5000\n",
      "\n",
      "    accuracy                           0.55     10000\n",
      "   macro avg       0.55      0.55      0.55     10000\n",
      "weighted avg       0.55      0.55      0.55     10000\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from sklearn.metrics import classification_report\n",
    "\n",
    "print(classification_report(y_valid, y_pred))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "60a82391",
   "metadata": {},
   "source": [
    "# LASER-based model\n",
    "\n",
    "> For the bilingual detection task, we extract the representation of the source and target sentences and\n",
    "tie them into one vector by taking their absolute difference and dot product, and adding them. This\n",
    "tied representation is then passed through **3 hidden layers of size 512, 150 and 75 respectively with\n",
    "dropout (Srivastava et al., 2014) of 50%, and then fed into a relu (Nair and Hinton, 2010) activation\n",
    "function, whose output is finally passed to the sigmoid function**. For the monolingual task, we just use\n",
    "the LASER French (target) representation of the sentence and pass it through the very same architecture.\n",
    "We train the classifiers with the **Adadelta optimizer with gradient clipping (clip value 3)**."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "38928ec9",
   "metadata": {},
   "outputs": [],
   "source": [
    "from laserembeddings import Laser\n",
    "\n",
    "def embed_with_laser(sents, lang=\"en\"):\n",
    "    laser = Laser()\n",
    "    return laser.embed_sentences(sents, lang=lang)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "f0dd86c4",
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train_laser = embed_with_laser(X_train)\n",
    "X_valid_laser = embed_with_laser(X_valid)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "4ae98050",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "\n",
    "model = nn.Sequential(\n",
    "    nn.Linear(1024, 512),\n",
    "    nn.Dropout(0.5),\n",
    "    nn.ReLU(),\n",
    "    nn.Linear(512, 150),\n",
    "    nn.Dropout(0.5),\n",
    "    nn.ReLU(),\n",
    "    nn.Linear(150, 75),\n",
    "    nn.Dropout(0.5),\n",
    "    nn.ReLU(),\n",
    "    nn.Linear(75, 1),\n",
    "    nn.Sigmoid()\n",
    ").to(\"cuda:0\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "8e546f51",
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch.utils.data import TensorDataset, DataLoader\n",
    "\n",
    "train_ds = TensorDataset(torch.from_numpy(X_train_laser).to(\"cuda:0\"), torch.from_numpy(y_train).to(\"cuda:0\"))\n",
    "valid_ds = TensorDataset(torch.from_numpy(X_valid_laser).to(\"cuda:0\"), torch.from_numpy(y_valid).to(\"cuda:0\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "2ce56315",
   "metadata": {},
   "outputs": [],
   "source": [
    "def evaluate(model, valid_loader):\n",
    "    model.eval()\n",
    "    correct, total = 0, 0\n",
    "    with torch.no_grad():\n",
    "        for (X, y) in valid_loader:\n",
    "            outputs = model(X).round()\n",
    "            batch_correct = (outputs == y.unsqueeze(-1)).sum()\n",
    "            batch_total = X.size(0)\n",
    "            correct += batch_correct\n",
    "            total += batch_total\n",
    "    print(\"Accuracy: {0:.4f}\".format(correct / total))\n",
    "    model.train()\n",
    "            \n",
    "        \n",
    "def train(model, train_loader, valid_loader, opt, num_epochs, log_every=5):\n",
    "    criterion = nn.BCELoss()\n",
    "    for epoch in range(num_epochs):\n",
    "\n",
    "        running_loss = 0.0\n",
    "        for i, (inputs, labels) in enumerate(train_loader, 1):\n",
    "\n",
    "            opt.zero_grad()\n",
    "\n",
    "            outputs = model(inputs)\n",
    "            loss = criterion(outputs, labels.unsqueeze(-1).float())\n",
    "            loss.backward()\n",
    "            nn.utils.clip_grad_norm_(model.parameters(), 3.0)\n",
    "            opt.step()\n",
    "\n",
    "            # print statistics\n",
    "            running_loss += loss.item()\n",
    "        if epoch % log_every == log_every - 1:\n",
    "            print(\"Epoch {0} loss: {1:.4f}\".format(epoch, running_loss))\n",
    "            evaluate(model, valid_loader)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "564b3fb6",
   "metadata": {},
   "outputs": [],
   "source": [
    "opt = torch.optim.Adadelta(model.parameters())\n",
    "train_loader = DataLoader(train_ds, batch_size=128, shuffle=True)\n",
    "valid_loader = DataLoader(valid_ds, batch_size=256)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "2d8424aa",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 4 loss: 271.0502\n",
      "Accuracy: 0.5000\n",
      "Epoch 9 loss: 271.0195\n",
      "Accuracy: 0.5000\n",
      "Epoch 14 loss: 270.9592\n",
      "Accuracy: 0.5248\n",
      "Epoch 19 loss: 269.2780\n",
      "Accuracy: 0.5106\n",
      "Epoch 24 loss: 266.7996\n",
      "Accuracy: 0.5718\n",
      "Epoch 29 loss: 264.8305\n",
      "Accuracy: 0.5790\n",
      "Epoch 34 loss: 263.6776\n",
      "Accuracy: 0.5560\n",
      "Epoch 39 loss: 263.3049\n",
      "Accuracy: 0.5815\n",
      "Epoch 44 loss: 262.0356\n",
      "Accuracy: 0.5731\n",
      "Epoch 49 loss: 261.2726\n",
      "Accuracy: 0.5828\n",
      "Epoch 54 loss: 260.4928\n",
      "Accuracy: 0.5737\n",
      "Epoch 59 loss: 259.7228\n",
      "Accuracy: 0.5826\n",
      "Epoch 64 loss: 259.2035\n",
      "Accuracy: 0.5878\n",
      "Epoch 69 loss: 258.0294\n",
      "Accuracy: 0.5824\n",
      "Epoch 74 loss: 257.0403\n",
      "Accuracy: 0.5842\n"
     ]
    }
   ],
   "source": [
    "train(model, train_loader, valid_loader, opt, 75)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d882bb49",
   "metadata": {},
   "source": [
    "# Transformer-based experiment\n",
    "\n",
    "The authors focus primarily on translating _out of_ English instead of _into_ English, so their choice of pretrained transformers is based on availability of target-language models. Instead, this demonstration focuses on translating Danish to English, so we can use all of our favorite English-language pretrained models.\n",
    "\n",
    "Here we show a proof-of-concept finetuning the RoBERTa base model for 3 epochs."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "f661a378",
   "metadata": {},
   "outputs": [],
   "source": [
    "from simpletransformers.classification import ClassificationModel, ClassificationArgs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "c66c5aa6",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of the model checkpoint at roberta-base were not used when initializing RobertaForSequenceClassification: ['roberta.pooler.dense.weight', 'lm_head.dense.bias', 'lm_head.decoder.weight', 'lm_head.bias', 'lm_head.dense.weight', 'roberta.pooler.dense.bias', 'lm_head.layer_norm.bias', 'lm_head.layer_norm.weight']\n",
      "- This IS expected if you are initializing RobertaForSequenceClassification from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing RobertaForSequenceClassification from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
      "Some weights of RobertaForSequenceClassification were not initialized from the model checkpoint at roberta-base and are newly initialized: ['classifier.dense.weight', 'classifier.out_proj.bias', 'classifier.out_proj.weight', 'classifier.dense.bias']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "cd6ae4ff00b94727a6aab6431ace9fae",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/50000 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Elijah\\miniconda3\\envs\\hont\\lib\\site-packages\\transformers\\optimization.py:306: FutureWarning: This implementation of AdamW is deprecated and will be removed in a future version. Use thePyTorch implementation torch.optim.AdamW instead, or set `no_deprecation_warning=True` to disable this warning\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "34a1ac6270bd431bace8e8a9627d5399",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Epoch:   0%|          | 0/1 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "e32af026cccf4fb9867bd4b7921cb1a5",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Running Epoch 0 of 1:   0%|          | 0/391 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "c5ac795fcbec40e4b2fd89d05bf87bd0",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/10000 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "92b3f151de0b49448a8a183a7b691bb1",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Running Evaluation:   0%|          | 0/40 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "import sklearn\n",
    "\n",
    "train_df = pd.DataFrame({\"text\": X_train, \"labels\": y_train})\n",
    "eval_df = pd.DataFrame({\"text\": X_valid, \"labels\": y_valid})\n",
    "\n",
    "\n",
    "model_args = ClassificationArgs(num_train_epochs=1, train_batch_size=128, eval_batch_size=256, overwrite_output_dir=True)\n",
    "model = ClassificationModel(\"roberta\", \"roberta-base\", args=model_args)\n",
    "\n",
    "model.train_model(train_df, acc=sklearn.metrics.accuracy_score)\n",
    "\n",
    "result, model_outputs, wrong_predictions = model.eval_model(eval_df, acc=sklearn.metrics.accuracy_score)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "e0385432",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'acc': 0.6594,\n",
      " 'auprc': 0.7502836292673939,\n",
      " 'auroc': 0.72908448,\n",
      " 'eval_loss': 0.6080136880278587,\n",
      " 'fn': 2619,\n",
      " 'fp': 787,\n",
      " 'mcc': 0.3426271720808088,\n",
      " 'tn': 4213,\n",
      " 'tp': 2381}\n"
     ]
    }
   ],
   "source": [
    "from pprint import pprint\n",
    "\n",
    "pprint(result)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ca2dc401",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
